{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import sys\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "import shutil\n",
    "from torchvision import transforms as T\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets all hyper-parameters options, and creates all of their combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomav\\Documents\\GitHub\\cs236781-project\\data/nyuv2\n"
     ]
    }
   ],
   "source": [
    "OVERFITTING_TRAINING     = True\n",
    "OVERFITTING_TRAINING     = False\n",
    "\n",
    "hyperparameters_filename = 'hyperparameters'\n",
    "overfit_data_dir_path    = 'data_overfit/nyuv2'\n",
    "normal_data_dir_path     = 'data/nyuv2'\n",
    "\n",
    "IMAGE_SIZES              = [(224,224), (64, 64)] #(448, 448)\n",
    "TRAIN_TEST_RATIOS        = [0.9]                                   # <-- MUST USE 0.5 FOR OVERFITTING\n",
    "BATCH_SIZES              = [4]\n",
    "NUM_WORKERSES            = [4]\n",
    "\n",
    "BETASES                  = [(0.9, 0.99)]\n",
    "LRS                      = [0.001]\n",
    "MOMENTUMS                = [0.9]\n",
    "WEIGHT_DECAYS            = [0.0005]\n",
    "\n",
    "STEP_SIZES               = [1000]\n",
    "GAMMAS                   = [0.1]\n",
    "\n",
    "NUM_EPOCHSES             = [10]\n",
    "\n",
    "all_combintations = list(itertools.product(*[IMAGE_SIZES, TRAIN_TEST_RATIOS, BATCH_SIZES, \\\n",
    "                                             NUM_WORKERSES, BETASES, LRS, MOMENTUMS, \\\n",
    "                                             WEIGHT_DECAYS, STEP_SIZES, GAMMAS, \\\n",
    "                                             NUM_EPOCHSES]))\n",
    "CWD                 = os.getcwd()\n",
    "if OVERFITTING_TRAINING:\n",
    "    DATASET_DIR     = os.path.join(CWD, overfit_data_dir_path)\n",
    "else:\n",
    "    DATASET_DIR     = os.path.join(CWD, normal_data_dir_path)\n",
    "print(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loops over all the combinations, trains, and saves both the models, and their hyper-parameters files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Hyper-Parameters:\n",
      "IMAGE_SIZE=(224, 224)\n",
      "TRAIN_TEST_RATIO=0.9\n",
      "BATCH_SIZE=4\n",
      "NUM_WORKERS=4\n",
      "\n",
      "BETAS=(0.9, 0.99)\n",
      "LR=0.001\n",
      "MOMENTUM=0.9\n",
      "WEIGHT_DECAY=0.0005\n",
      "\n",
      "STEP_SIZE=1000\n",
      "GAMMA=0.1\n",
      "\n",
      "NUM_EPOCHS=10\n",
      "\n",
      "Found 1449 images in dataset folder.\n",
      "[I] - default optimizer set: SGD(lr=0.001,momentum=0.9,weight_decay=0.0005)\n",
      "[I] - default scheduler set: StepSR(step_size=1000,gamma=0.1)\n",
      "--- EPOCH 1/10 ---\n",
      "train_batch (0.069):   1%|â–Œ                                                            | 3/327 [00:04<08:06,  1.50s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-322c9eb28e1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{checkpoint_file}.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{checkpoint_file}.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoints\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;31m# Save the current hyper-parameters file next to the current saved model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\cs236781-project\\train.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dl_train, dl_test, checkpoints, early_stopping, print_every, post_epoch_fn, **kw)\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'--- EPOCH {epoch+1}/{self.num_epochs} ---'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Conditional verbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[0mtrain_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m             \u001b[0mtest_result\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0mtrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\cs236781-project\\train.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(self, dl_train, **kw)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;31m#  self.model.set_requires_grad(True) (manorz, 03/14/20)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# set train mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_foreach_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtest_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_test\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mEpochResult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\cs236781-project\\train.py\u001b[0m in \u001b[0;36m_foreach_batch\u001b[1;34m(dl, forward_fn, verbose, max_batches)\u001b[0m\n\u001b[0;32m    215\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                 \u001b[0mbatch_res\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mpbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{pbar_name} ({batch_res.loss:.3f})'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\cs236781-project\\train.py\u001b[0m in \u001b[0;36mtrain_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mBatchResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtest_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mBatchResult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "current_model_id = 0\n",
    "for combintation in all_combintations:\n",
    "    IMAGE_SIZE       = combintation[0]\n",
    "    TRAIN_TEST_RATIO = combintation[1]\n",
    "    BATCH_SIZE       = combintation[2]\n",
    "    NUM_WORKERS      = combintation[3]\n",
    "\n",
    "    BETAS            = combintation[4]\n",
    "    LR               = combintation[5]\n",
    "    MOMENTUM         = combintation[6]\n",
    "    WEIGHT_DECAY     = combintation[7]\n",
    "\n",
    "    STEP_SIZE        = combintation[8]\n",
    "    GAMMA            = combintation[9]\n",
    "\n",
    "    NUM_EPOCHS       = combintation[10]\n",
    "\n",
    "    with open(hyperparameters_filename + '.py', \"w\") as hyperparameters_file:\n",
    "        print(f\"IMAGE_SIZE={IMAGE_SIZE}\", file=hyperparameters_file)\n",
    "        print(f\"TRAIN_TEST_RATIO={TRAIN_TEST_RATIO}\", file=hyperparameters_file)\n",
    "        print(f\"BATCH_SIZE={BATCH_SIZE}\", file=hyperparameters_file)\n",
    "        print(f\"NUM_WORKERS={NUM_WORKERS}\", file=hyperparameters_file)\n",
    "        print(f\"\", file=hyperparameters_file)\n",
    "        print(f\"BETAS={BETAS}\", file=hyperparameters_file)\n",
    "        print(f\"LR={LR}\", file=hyperparameters_file)\n",
    "        print(f\"MOMENTUM={MOMENTUM}\", file=hyperparameters_file)\n",
    "        print(f\"WEIGHT_DECAY={WEIGHT_DECAY}\", file=hyperparameters_file)\n",
    "        print(f\"\", file=hyperparameters_file)\n",
    "        print(f\"STEP_SIZE={STEP_SIZE}\", file=hyperparameters_file)\n",
    "        print(f\"GAMMA={GAMMA}\", file=hyperparameters_file)\n",
    "        print(f\"\", file=hyperparameters_file)\n",
    "        print(f\"NUM_EPOCHS={NUM_EPOCHS}\", file=hyperparameters_file)\n",
    "\n",
    "    # import here, to include the above new hyper-parameters file.\n",
    "    from models import SpecialFuseNetModel\n",
    "    from data_manager import rgbd_gradients_dataset, rgbd_gradients_dataloader\n",
    "    from train import FuseNetTrainer\n",
    "\n",
    "    print(\"Current Hyper-Parameters:\")\n",
    "    with open(hyperparameters_filename + '.py', \"r\") as hyperparameters_file:\n",
    "        print(hyperparameters_file.read())\n",
    "    \n",
    "    rgbd_grads_ds = rgbd_gradients_dataset(root=DATASET_DIR, use_transforms=True)\n",
    "\n",
    "    dl_train,dl_test = rgbd_gradients_dataloader(root=DATASET_DIR, use_transforms=True)\n",
    "\n",
    "    # _ = plot.rgbd_gradients_dataset_first_n(dataset=rgbd_grads_ds,n=5)\n",
    "    print(f'Found {len(rgbd_grads_ds)} images in dataset folder.')\n",
    "\n",
    "    sample_batch = next(iter(dl_train))\n",
    "    rgb_size = tuple(sample_batch['rgb'].shape[1:])\n",
    "    depth_size = tuple(sample_batch['depth'].shape[1:])\n",
    "    grads_size = tuple(sample_batch['x'].shape[1:])\n",
    "\n",
    "    # Train\n",
    "    fusenetmodel = SpecialFuseNetModel(rgb_size=rgb_size,depth_size=depth_size,grads_size=grads_size, device=device)\n",
    "    trainer = FuseNetTrainer(model=fusenetmodel, device=device)\n",
    "    checkpoint_folder = 'checkpoints/'\n",
    "    checkpoint_file_name = 'special_fusenet_' + str(current_model_id)\n",
    "    checkpoint_file = checkpoint_folder + checkpoint_file_name\n",
    "    if os.path.isfile(f'{checkpoint_file}.pt'):\n",
    "        os.remove(f'{checkpoint_file}.pt')\n",
    "    res = trainer.fit(dl_train, dl_test, early_stopping=1000, print_every=10, checkpoints=checkpoint_file)\n",
    "    \n",
    "    # Save the current hyper-parameters file next to the current saved model.\n",
    "    shutil.move(CWD + '/' + hyperparameters_filename + '.py', CWD + '/' + checkpoint_folder + hyperparameters_filename + '_' + checkpoint_file_name + '.py')\n",
    "    \n",
    "    print(\"-----------------------------------------------------------------\")\n",
    "    current_model_id += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
